{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an LSTM from Scratch\n",
    "\n",
    "Adapted from https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n",
    "And https://gist.github.com/piEsposito/a05bc12cd107fdec68e35ad61302da4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "np.set_printoptions(formatter={'float_kind': \"{:.3f}\".format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Written by Pi Esposito\n",
    "    https://gist.github.com/piEsposito/a05bc12cd107fdec68e35ad61302da4c\n",
    "    \"\"\"\n",
    "    def __init__(self, input_sz, hidden_sz):\n",
    "        super().__init__()\n",
    "        self.input_sz = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
    "        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))\n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, \n",
    "                init_states=None):\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device), \n",
    "                        torch.zeros(bs, self.hidden_size).to(x.device))\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "         \n",
    "        HS = self.hidden_size\n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            # batch the computations into a single matrix multiplication\n",
    "            gates = x_t @ self.W + h_t @ self.U + self.bias\n",
    "            i_t, f_t, g_t, o_t = (\n",
    "                torch.sigmoid(gates[:, :HS]),     # input\n",
    "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
    "                torch.tanh(gates[:, HS*2:HS*3]),  # g(X, h)\n",
    "                torch.sigmoid(gates[:, HS*3:]),   # output\n",
    "            )\n",
    "            c_t = f_t * c_t + i_t * g_t  # forget old, add new\n",
    "            h_t = o_t * torch.tanh(c_t)  # produce output\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)\n",
    "    \n",
    "    def verbose_forward(self, x):\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device), \n",
    "                    torch.zeros(bs, self.hidden_size).to(x.device))\n",
    "         \n",
    "        (i_t_vals, f_t_vals, g_t_vals,\n",
    "         c_t_vals, h_t_vals, o_t_vals) = ([], [], [], [], [], [])\n",
    "        \n",
    "        HS = self.hidden_size\n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            # batch the computations into a single matrix multiplication\n",
    "            gates = x_t @ self.W + h_t @ self.U + self.bias\n",
    "            i_t, f_t, g_t, o_t = (\n",
    "                torch.sigmoid(gates[:, :HS]),     # input\n",
    "                torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
    "                torch.tanh(gates[:, HS*2:HS*3]),  # g(X, h)\n",
    "                torch.sigmoid(gates[:, HS*3:]),   # output\n",
    "            )\n",
    "            c_t = f_t * c_t + i_t * g_t  # forget old, add new\n",
    "            h_t = o_t * torch.tanh(c_t)  # produce output\n",
    "            \n",
    "            i_t_vals.append(i_t.detach().numpy())\n",
    "            f_t_vals.append(f_t.detach().numpy())\n",
    "            g_t_vals.append(g_t.detach().numpy())\n",
    "            c_t_vals.append(c_t.detach().numpy())\n",
    "            h_t_vals.append(h_t.detach().numpy())\n",
    "            o_t_vals.append(o_t.detach().numpy())\n",
    "\n",
    "        return {\"i_t\": i_t_vals, \"f_t\": f_t_vals,\n",
    "                \"g_t\": g_t_vals, \"c_t\": c_t_vals,\n",
    "                \"h_t\": h_t_vals, \"o_t\": o_t_vals,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([0]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_onehot(arr, vocab_size):\n",
    "    # Convert to one-hot representation\n",
    "    # https://en.wikipedia.org/wiki/One-hot\n",
    "\n",
    "    n, max_length = arr.shape\n",
    "    onehot_data = np.zeros([n, max_length, vocab_size])\n",
    "    for v in range(vocab_size):\n",
    "        onehot_row = np.zeros([vocab_size])\n",
    "        onehot_row[v] = 1\n",
    "        onehot_data[arr == v] = onehot_row\n",
    "\n",
    "    return onehot_data\n",
    "\n",
    "class CountingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, n, max_length=8, vocab_size=8):\n",
    "        \n",
    "        assert vocab_size > 2\n",
    "        self.n = n\n",
    "        self.vocab_size = vocab_size\n",
    "        seq_lengths = np.random.randint(max_length // 2, max_length, n)\n",
    "        data = np.random.randint(0, vocab_size, [n, max_length])\n",
    "        \n",
    "        # Replace elements past the sequence length with -1\n",
    "        for i in range(n):\n",
    "            data[i, slice(seq_lengths[i] + 1, None)] = -1\n",
    "            \n",
    "        onehot_data = make_onehot(data, vocab_size)\n",
    "        \n",
    "        # Label is whether ones outnumber twos in the sequence\n",
    "        num_ones = (data == 1).sum(axis=1, keepdims=True)\n",
    "        num_twos = (data == 2).sum(axis=1, keepdims=True)\n",
    "        label = (num_ones > num_twos).astype(int)\n",
    "\n",
    "        self.data = torch.tensor(onehot_data).float()\n",
    "        self.label = torch.tensor(label).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, item_index):\n",
    "        \"\"\"\n",
    "        Allow us to select items with `dataset[0]`\n",
    "        Returns (x, y)\n",
    "            x: the data tensor\n",
    "            y: the label tensor\n",
    "        \"\"\"\n",
    "        return self.data[item_index], self.label[item_index]\n",
    "    \n",
    "d = CountingDataset(3, max_length=8, vocab_size=3)\n",
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    2, Loss: 0.68721, Acc: 61.4, in 0.0min\n",
      "Epoch:    4, Loss: 0.67619, Acc: 61.6, in 0.1min\n",
      "Epoch:    6, Loss: 0.67887, Acc: 59.3, in 0.1min\n",
      "Epoch:    8, Loss: 0.64599, Acc: 63.6, in 0.1min\n",
      "Epoch:   10, Loss: 0.39516, Acc: 85.5, in 0.1min\n",
      "Epoch:   12, Loss: 0.20761, Acc: 92.3, in 0.1min\n",
      "Epoch:   14, Loss: 0.01162, Acc: 99.6, in 0.2min\n",
      "Epoch:   16, Loss: 0.00279, Acc: 100.0, in 0.2min\n",
      "Epoch:   18, Loss: 0.00028, Acc: 100.0, in 0.2min\n",
      "Epoch:   20, Loss: 0.00014, Acc: 100.0, in 0.2min\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "hidden_size = 2\n",
    "num_epochs = 20\n",
    "max_length = 10\n",
    "\n",
    "dataset = CountingDataset(\n",
    "    10000, max_length=max_length, vocab_size=vocab_size)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "lstm = CustomLSTM(vocab_size, hidden_size)\n",
    "# lstm = nn.LSTM(vocab_size, hidden_size)\n",
    "classifier = nn.Linear(hidden_size, 2)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "lstm_opt = torch.optim.SGD(lstm.parameters(), lr=1e-3)\n",
    "clf_opt = torch.optim.SGD(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "    for (X, y) in data_loader:\n",
    "\n",
    "        hidden, _ = lstm(X)\n",
    "        final_hidden = hidden[:, -1, :]\n",
    "        logits = classifier(final_hidden)\n",
    "\n",
    "        loss = loss_func(logits, y.squeeze())\n",
    "        acc = torch.argmax(logits, dim=1, keepdims=True) == y\n",
    "        acc = torch.mean(acc.float())\n",
    "\n",
    "        loss.backward()\n",
    "        lstm_opt.step()\n",
    "        clf_opt.step()\n",
    "\n",
    "        total_loss.append(loss.detach().numpy())\n",
    "        total_acc.append(acc.detach().numpy())\n",
    "\n",
    "    if (epoch + 1) % max(1, num_epochs // 10) == 0:\n",
    "        mins = (time.time() - start) / 60\n",
    "\n",
    "        print(\", \".join([\n",
    "            f\"Epoch: {epoch + 1:4d}\",\n",
    "            f\"Loss: {np.mean(total_loss):.5f}\",\n",
    "            f\"Acc: {100*np.mean(total_acc):.1f}\",\n",
    "            f\"in {mins:.1f}min\",\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forget gate mean: [1.000 0.291] std: [0.000 0.048]\n",
      "val       i_t           g_t           c_t           h_t     \n",
      " 1    0.95   0.14   1.00  -1.00   0.95  -0.14   0.74  -0.05\n",
      " 0    0.00   0.08   0.58  -0.67   0.95  -0.09   0.74  -0.01\n",
      " 0    0.00   0.08   0.55  -0.67   0.95  -0.08   0.74  -0.00\n",
      " 0    0.00   0.08   0.54  -0.67   0.95  -0.08   0.74  -0.00\n",
      " 0    0.00   0.08   0.54  -0.67   0.95  -0.08   0.74  -0.00\n",
      " 0    0.00   0.08   0.54  -0.67   0.95  -0.08   0.74  -0.00\n",
      " 0    0.00   0.08   0.54  -0.67   0.95  -0.08   0.74  -0.00\n",
      " 2    1.00   0.19  -1.00   0.92  -0.05   0.15  -0.05   0.03\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "X, y = dataset[i:i+1]\n",
    "\n",
    "X = np.array([[1., 0, 0, 0, 0, 0, 0, 2]])\n",
    "X = torch.tensor(make_onehot(X, vocab_size)).float()\n",
    "\n",
    "out = lstm.verbose_forward(X)\n",
    "mask = (torch.sum(X, axis=2) > 0).squeeze().numpy()\n",
    "\n",
    "val = np.argmax(X[:, mask, :], axis=2).reshape(-1, 1)\n",
    "i_t = np.array(out[\"i_t\"])[mask].squeeze()\n",
    "g_t = np.array(out[\"g_t\"])[mask].squeeze()\n",
    "f_t = np.array(out[\"f_t\"])[mask].squeeze()\n",
    "c_t = np.array(out[\"c_t\"])[mask].squeeze()\n",
    "h_t = np.array(out[\"h_t\"])[mask].squeeze()\n",
    "o_t = np.array(out[\"o_t\"])[mask].squeeze()\n",
    "\n",
    "print(\"forget gate mean: {} std: {}\".format(\n",
    "    np.mean(f_t, axis=0), np.std(f_t, axis=0)))\n",
    "\n",
    "columns = [\"i_t\", \"g_t\", \"c_t\", \"h_t\"]\n",
    "col_vals = [np.array(out[col])[mask].squeeze() for col in columns]\n",
    "table = np.concatenate([val] + col_vals, axis=1).tolist()\n",
    "\n",
    "fmt = \"{:^3s}  \" + \" \".join(\n",
    "    [\"{:^13s}\" for _ in range(len(columns))])\n",
    "print(fmt.format(\"val\", *columns))\n",
    "for i in range(len(table)):\n",
    "    row = table[i]\n",
    "    print(\"{:^3d}\".format(int(row[0])), end=\" \")\n",
    "    print(\" \".join(map(\"{:6.2f}\".format, row[1:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

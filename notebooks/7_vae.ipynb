{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b1cb09b-df94-4117-91ed-fe63d3f1edaf"
   },
   "source": [
    "# Notebook 7: Variational Autoencoders (VAEs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd021d50-eec4-4bc3-8b79-e3ed8c377ca0",
    "tags": []
   },
   "source": [
    "## __1.__ <a name=\"setup\">Setup</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2753e29-a91d-4eab-929f-dc6fdf7a4cd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zachwooddoughty/Documents/CS449Lectures/lectures/notebooks\n",
      "fatal: destination path 'lectures' already exists and is not an empty directory.\n",
      "/Users/zachwooddoughty/Documents/CS449Lectures/lectures/notebooks/lectures\n"
     ]
    }
   ],
   "source": [
    "# Choose basedir as either local or hosted directory\n",
    "import os\n",
    "if \"COLAB_BACKEND_VERSION\" in os.environ:\n",
    "    base_dir = \"/content\"\n",
    "else:\n",
    "    base_dir = os.getcwd()\n",
    "# get helper code from the course repository\n",
    "# install common packages used for deep learning\n",
    "%cd $base_dir\n",
    "!git clone https://github.com/cs449s23/lectures.git lectures/\n",
    "%cd $base_dir/lectures/\n",
    "!git pull -q origin main\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59411b2d-ee16-47df-8762-8d7b9383721a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from pathlib import Path\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.gan import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "691dc6eb-f818-4928-99bc-ffc807848dca"
   },
   "source": [
    "## __2.__ <a name=\"VAE\">A Simple MNIST VAE</a>\n",
    "\n",
    "In the previous notebook, we saw a \"standard\" autoencoder. Its encoder maps the input image into a latent space, and the decoder maps it back to the image space, but we don't put any constraints on the latent space itself.\n",
    "\n",
    "Over the past week, we've discussed variational inference and how they can inform our creation of a VAE, which is what we'll explore in this notebook.\n",
    "\n",
    "The following code is very similar to the autoencoder notebook from last week, but with a few crucial differences. First, our encoder outputs two tensors: `mu` and `log_var`. Our model then has a `sample` method which turns these two tensors into a `z` sample, which we then pass to our decoder as before. Finally, in addition to reconstruction loss, we are calculating the KL divergence between our latent space and a Normal distribution.\n",
    "\n",
    "Q: why do we have our encoder output the log of the variance, rather than just the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc4f6c18-8f95-40c4-aa91-0dc238d4af30"
   },
   "outputs": [],
   "source": [
    "class MLPVAEEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 number_of_hidden_layers: int, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int, \n",
    "                 latent_size: int,\n",
    "                 activation: torch.nn.Module):\n",
    "        \"\"\"Construct a simple MLP encoder\"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        assert number_of_hidden_layers >= 0, \"Encoder number_of_hidden_layers must be at least 0\"\n",
    "        \n",
    "        dims_in = [input_size] + [hidden_size] * (number_of_hidden_layers - 1)\n",
    "        dims_out = [hidden_size] * number_of_hidden_layers\n",
    "        layers = []\n",
    "        for i in range(number_of_hidden_layers):\n",
    "            layers.append(torch.nn.Linear(dims_in[i], dims_out[i]))\n",
    "            \n",
    "            if i < number_of_hidden_layers - 1:\n",
    "                layers.append(activation)\n",
    "\n",
    "        self.shared = torch.nn.Sequential(*layers)\n",
    "        self.mu = torch.nn.Linear(hidden_size, latent_size)\n",
    "        self.log_var = torch.nn.Linear(hidden_size, latent_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.shared(x)\n",
    "        return (self.mu(x), self.log_var(x))\n",
    "\n",
    "\n",
    "class MLPDecoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 number_of_hidden_layers: int, \n",
    "                 latent_size: int, \n",
    "                 hidden_size: int, \n",
    "                 input_size: int,\n",
    "                 activation: torch.nn.Module):\n",
    "        \"\"\"Construct a simple MLP decoder\"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        assert number_of_hidden_layers >= 0, \"Decoder number_of_hidden_layers must be at least 0\"\n",
    "        \n",
    "        dims_in = [latent_size] + [hidden_size] * number_of_hidden_layers\n",
    "        dims_out = [hidden_size] * number_of_hidden_layers + [input_size]  # final output is an image\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(number_of_hidden_layers + 1):\n",
    "            layers.append(torch.nn.Linear(dims_in[i], dims_out[i]))\n",
    "            \n",
    "            if i < number_of_hidden_layers:\n",
    "                layers.append(activation)\n",
    "        \n",
    "        # apply Tanh after final layer to bound pixels to range [-1, 1]\n",
    "        layers.append(torch.nn.Sigmoid())\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MLPVAE(torch.nn.Module):\n",
    "                          \n",
    "    def __init__(self, \n",
    "                 number_of_hidden_layers: int, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int, \n",
    "                 latent_size: int,\n",
    "                 activation_encoder: torch.nn.Module = torch.nn.ReLU(),\n",
    "                 activation_decoder: torch.nn.Module = torch.nn.LeakyReLU(0.2)\n",
    "                ):\n",
    "        \"\"\"Construct a simple MLP *variational* autoencoder\n",
    "        \n",
    "        number_of_hidden_layers: An int. Must be >=0. Defines the number of\n",
    "                hidden layers for both the encoder E and decoder D \n",
    "        latent_size:  An int. Defines the size of the latent representation produced by\n",
    "                      the encoder.\n",
    "        hidden_size: An int. The size of each hidden layer for the encoder E and\n",
    "                     the decoder D.\n",
    "        input_size: An int. Determines the size of the input and output images\n",
    "        activation_encoder: A torch.nn.Module defining the activation function in every \n",
    "                            hidden layer of the encoder.\n",
    "        activation_decoder: A torch.nn.Module defining the activation function in every \n",
    "                            hidden layer of the decoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "                          \n",
    "        self.encoder = MLPVAEEncoder(\n",
    "            number_of_hidden_layers=number_of_hidden_layers, \n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            latent_size=latent_size,\n",
    "            activation=activation_encoder\n",
    "        )\n",
    "        \n",
    "        self.decoder = MLPDecoder(\n",
    "            number_of_hidden_layers=number_of_hidden_layers, \n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size,\n",
    "            latent_size=latent_size,\n",
    "            activation=activation_decoder\n",
    "        )    \n",
    "\n",
    "    def sample(self, mu: torch.Tensor, log_var: torch.Tensor):\n",
    "        std = torch.exp(log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "\n",
    "    def kl_divergence(self, mu: torch.Tensor, log_var: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute KL(N(mu, var) || N(0, 1))\n",
    "        \"\"\"\n",
    "        return -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "    def encode(self, x: torch.Tensor):\n",
    "        return self.encoder(x)\n",
    "        \n",
    "    def decode(self, z: torch.Tensor):\n",
    "        return self.decoder(z)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.sample(mu, log_var)\n",
    "        kl = self.kl_divergence(mu, log_var)\n",
    "        return self.decode(z), kl\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "708ab262-0c0d-4cae-8d91-7b0b4ea41c6c",
    "tags": []
   },
   "source": [
    "#### Training Configuration\n",
    "\n",
    "To view progress after starting your training loop below, you may need to refresh Tensorboard (an icon should appear in the top-right corner of the following cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8a9c0b28-e14e-40b8-8fb8-f9e8790026f4"
   },
   "outputs": [],
   "source": [
    "# here, we'll initialize TensorBoard. You should see an empty window in this cell, which will populate with\n",
    "# graphs as soon as we run our training code below.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31128e75-9afb-4598-9f24-9e4cb40a07bc"
   },
   "outputs": [],
   "source": [
    "# autoencoder training hyperparameters\n",
    "image_size = 28\n",
    "batch_size = 64\n",
    "latent_size = 64\n",
    "hidden_size = 256\n",
    "number_of_hidden_layers = 2\n",
    "lr = 0.0002\n",
    "\n",
    "# fix random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load MNIST dataset\n",
    "mnist = load_mnist(batch_size=batch_size)\n",
    "\n",
    "# initialize the model\n",
    "model = MLPVAE(\n",
    "    number_of_hidden_layers=number_of_hidden_layers, \n",
    "    latent_size=latent_size, \n",
    "    hidden_size=hidden_size, \n",
    "    input_size=image_size*image_size, \n",
    ").to(device)\n",
    "\n",
    "# use an optimizer to handle parameter updates\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# loss function 1: mean-squared error between original and reconstructed images\n",
    "recons_loss = torch.nn.MSELoss()\n",
    "\n",
    "# save all log data to a local directory\n",
    "run_dir = \"logs\"\n",
    "\n",
    "# to clear out TensorBoard and start totally fresh, we'll need to\n",
    "# remove old logs by deleting them from the directory\n",
    "# !rm -rf ./logs/\n",
    "\n",
    "# timestamp the logs for each run so we can sort through them \n",
    "run_time = datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\")\n",
    "\n",
    "# initialize a SummaryWriter object to handle all logging actions\n",
    "logger = SummaryWriter(log_dir=Path(run_dir) / run_time, flush_secs=20)\n",
    "\n",
    "# log reconstructions of a fixed set of images during training\n",
    "example_batch, _ = next(iter(mnist))\n",
    "example_batch = example_batch.to(device)\n",
    "logger.add_image(\"training_images\", make_grid(example_batch, math.floor(math.sqrt(batch_size))), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YdTVocd0lkx"
   },
   "source": [
    "### Vanishing KL\n",
    "\n",
    "When we looked at GAN models, there was a need for balance between the discriminator and the generator. Neither can learn to improve if the other does not. There's a related issue with VAEs because the KL loss and the reconstruction loss are in conflict; it's very easy to set the KL loss to zero by encoding every X as a standard Gaussian, but that provides the decoder with no information.\n",
    "\n",
    "In practice, this means it can be necessary to reduce the weight we put on the KL loss term. That is, we'll train each epoch with the form `loss = recons_loss + kl_weight * kl_loss`, and we'll set `kl_weight` to be something small, so the optimizer decides to care more about `recons_loss`.\n",
    "\n",
    "The following function allows our autoencoder to get a \"head start\" on reconstruction loss by ignoring KL entirely for `wait` epochs, and then slowly increasing it. For a discussion of more complex ways to handle this KL weight, see [this paper](https://aclanthology.org/N19-1021/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvROj2BsKIL2"
   },
   "outputs": [],
   "source": [
    "def get_kl_weight(epoch, epochs, wait=3, base=0.1):\n",
    "    \"\"\"\n",
    "    For the first `wait` epochs, ignore KL loss entirely.\n",
    "    Then have it linearly increase from 0 to `base` as\n",
    "    epochs increase up to the maximum `epochs`.\n",
    "    \"\"\"\n",
    "    if epoch < wait:\n",
    "        return 0\n",
    "    return base * (epoch - wait) / (epochs - wait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b605afd-373a-4ade-b056-129e815f1be1",
    "tags": []
   },
   "source": [
    "#### Training Loop\n",
    "\n",
    "We're ready to train! Note that we've rewritten `model.foward` to return a tuple of both reconstructed images and our KL loss. And make sure you can follow how we're using `get_kl_weight` to weight that KL loss term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c89c4c4-226e-4031-a1ed-14d7352777a6"
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # weight batch losses/scores proportional to batch size\n",
    "    iter_count = 0\n",
    "    kl_loss_epoch = 0\n",
    "    recons_loss_epoch = 0\n",
    "    \n",
    "    kl_weight = get_kl_weight(epoch, epochs, base=1e-5)\n",
    "    for batch_idx, batch_data in enumerate(mnist):\n",
    "\n",
    "        # we only care about inputs, not labels\n",
    "        x_real, _ = batch_data\n",
    "        \n",
    "        # flatten input images and move to device\n",
    "        x_real = x_real.to(device)\n",
    "        n_batch = x_real.shape[0]\n",
    "        x_real = x_real.reshape(n_batch, -1)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        # train on a batch of inputs\n",
    "        x_reconstructed, batch_kl_loss = model(x_real)\n",
    "        batch_recons_loss = recons_loss(x_real, x_reconstructed)\n",
    "\n",
    "        total_loss = kl_weight * batch_kl_loss + batch_recons_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # log loss\n",
    "        kl_loss_epoch += batch_kl_loss.detach().item() * n_batch\n",
    "        recons_loss_epoch += batch_recons_loss.detach().item() * n_batch\n",
    "        iter_count += n_batch\n",
    "    \n",
    "    # plot loss\n",
    "    kl_loss_epoch /= iter_count\n",
    "    logger.add_scalar(\"kl_loss\", kl_loss_epoch, epoch)\n",
    "    recons_loss_epoch /= iter_count\n",
    "    logger.add_scalar(\"recons_loss\", recons_loss_epoch, epoch)\n",
    "    \n",
    "    if (epoch + 1) % max(1, epochs // 10) == 0:\n",
    "        minutes = (time.time() - start_time) / 60\n",
    "        log = \"\\t\".join([\n",
    "            f\"Epoch: {epoch + 1}\",\n",
    "            f\"MSE Loss: {recons_loss_epoch :0.4f}\",\n",
    "            f\"KL Loss: {kl_loss_epoch:7.1f}\",\n",
    "            f\"KL Weight: {kl_weight:0.1e}\",\n",
    "            f\"in {minutes:.1f} min.\",\n",
    "        ])\n",
    "        print(log)\n",
    "        reconstructed_batch = model(\n",
    "            example_batch.reshape(batch_size, -1))[0].reshape(batch_size, 1, image_size, image_size)\n",
    "        logger.add_image(\n",
    "            \"reconstructed_images\",\n",
    "            make_grid(reconstructed_batch, math.floor(math.sqrt(batch_size)), title=\"Reconstructed Images\"),\n",
    "            epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c94d131-291a-4e5c-ad5a-101540dd6f14"
   },
   "source": [
    "You should be able to see that the MSE and KL loss are in tension; as we increase the KL weight, the MSE loss starts to increase.\n",
    "\n",
    "Let's now look at what our model's reconstructions look like. Are they worse than the reconstructions we saw with our standard autoencoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "289d82f1-307c-460b-bde1-1a5a2b0e0431"
   },
   "outputs": [],
   "source": [
    "example_batch, _ = next(iter(mnist))\n",
    "example_batch = example_batch.to(device)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n",
    "\n",
    "indices = np.argsort(np.random.rand(example_batch.shape[0]))[:3]\n",
    "\n",
    "for col, index in enumerate(indices):\n",
    "\n",
    "    example_image = example_batch[index].cpu()\n",
    "    axes[0, col].imshow(example_image.squeeze(), cmap='gray')\n",
    "    axes[0, col].set_title(\"Original Image\")\n",
    "    axes[0, col].axis('off')\n",
    "\n",
    "    noisy_image = example_image + torch.randn_like(example_image) * 0.1\n",
    "    axes[1, col].imshow(noisy_image.squeeze(), cmap='gray')\n",
    "    axes[1, col].set_title(\"Noisy Image\")\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "    noisy_image_input = noisy_image.flatten().unsqueeze(0).to(device)\n",
    "    reconstructed_image = model(noisy_image_input)[0].reshape(1, image_size, image_size).detach().cpu()\n",
    "    axes[2, col].imshow(reconstructed_image.squeeze(), cmap='gray')\n",
    "    axes[2, col].set_title(\"Reconstructed Noisy Image\")\n",
    "    axes[2, col].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abwDyeOTx1uC"
   },
   "outputs": [],
   "source": [
    "# Save the 60,000 z values for our MNIST data\n",
    "\n",
    "z_values = []\n",
    "\n",
    "for batch_idx, batch_data in enumerate(mnist):\n",
    "\n",
    "    # we only care about inputs, not labels\n",
    "    x_real, _ = batch_data\n",
    "    \n",
    "    # flatten input images and move to device\n",
    "    x_real = x_real.to(device)\n",
    "    n_batch = x_real.shape[0]\n",
    "    x_real = x_real.reshape(n_batch, -1)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    # train on a batch of inputs\n",
    "    z = model.encoder(x_real)[0].cpu().detach().numpy()\n",
    "    z_values.append(z)\n",
    "\n",
    "z_values = np.concatenate(z_values, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_il403y4bVu"
   },
   "source": [
    "Now let's try to explore the latent space that our VAE has learned. We'll do the same thing we did for our standard autoencoder, except we'll sample according to the variance that our encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YMul9px7l6-"
   },
   "outputs": [],
   "source": [
    "z_std = np.std(z_values)\n",
    "\n",
    "# Grab a random image\n",
    "seed = np.random.randint(10000)\n",
    "print(seed)\n",
    "# np.random.seed(1033)  # 8138, 5360, \n",
    "index = np.argsort(np.random.rand(example_batch.shape[0]))[0]\n",
    "example_image = example_batch[index].cpu()\n",
    "plt.imshow(example_image.reshape(image_size, image_size), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Find its latent representation in our autoencoder\n",
    "image_input = example_image.flatten().unsqueeze(0).to(device)\n",
    "mu, log_var = model.encoder(image_input)\n",
    "mu = mu.detach().cpu()\n",
    "log_var = log_var.detach().cpu()\n",
    "\n",
    "# # You can print out the mean and variance of the Gaussian.\n",
    "# print(torch.round(mu, decimals=4))\n",
    "# print(log_var.exp())\n",
    "\n",
    "n = 7\n",
    "fig, axes = plt.subplots(nrows=n, ncols=n, figsize=(12, 12))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        \n",
    "        midpoint = (n - 1) // 2\n",
    "        if i == midpoint and j == midpoint:\n",
    "            offset = torch.zeros_like(mu)\n",
    "        else:\n",
    "            offset = torch.normal(mean=0, std=log_var.exp())\n",
    "#             offset = torch.normal(mean=torch.zeros_like(z), std=1 * z_std)\n",
    "\n",
    "        new_z = mu + offset.float()\n",
    "\n",
    "        new_image = model.decoder(new_z.to(device))\n",
    "        new_image = new_image.reshape(1, image_size, image_size).detach().cpu()\n",
    "        axes[i, j].imshow(new_image.squeeze(), cmap='gray')\n",
    "        # axes[i, j].set_title(f\"({xval:.2f}, {yval:.2f})\")\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fr2m-q-syQIK"
   },
   "outputs": [],
   "source": [
    "# Do PCA on those z values\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "z2 = pca.fit_transform(z_values)\n",
    "\n",
    "z2_percentiles = np.percentile(z2, [2.5, 97.5], axis=0)\n",
    "z2_0_low, z2_1_low, z2_0_high, z2_1_high = z2_percentiles.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2BcTcpd6dQq"
   },
   "source": [
    "We'll now use PCA to condense our latent space into two dimensions and visualize it. If you increase `n` and/or `dim_mult`, you can see more and/or a wider range of sampled images around the initially-encoded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2W1ZX0Xt7iT"
   },
   "outputs": [],
   "source": [
    "# Grab a random image\n",
    "seed = np.random.randint(10000)\n",
    "print(seed)\n",
    "# 5854, 6616, \n",
    "np.random.seed(seed)\n",
    "index = np.argsort(np.random.rand(example_batch.shape[0]))[0]\n",
    "example_image = example_batch[index].cpu()\n",
    "plt.imshow(example_image.reshape(image_size, image_size), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Find its latent representation in our autoencoder\n",
    "image_input = example_image.flatten().unsqueeze(0).to(device)\n",
    "z = model.encoder(image_input)[0].detach().cpu()\n",
    "z_pca = pca.transform(z.numpy())[0]\n",
    "\n",
    "# Try increasing dim_mult and/or n\n",
    "n = 11\n",
    "dim_mult = 4\n",
    "\n",
    "dim0_offset = dim_mult * np.min((z_pca[0] - z2_0_low, z2_0_high - z_pca[0]))\n",
    "dim1_offset = dim_mult * np.min((z_pca[1] - z2_1_low, z2_1_high - z_pca[1]))\n",
    "\n",
    "print(\"Initial image at \", z_pca)\n",
    "print(f\"Varying dim0 +/- {dim0_offset:.1f} and dim1 +/- {dim1_offset:.1f}\")\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n, ncols=n, figsize=(20, 20))\n",
    "\n",
    "for i, xval in enumerate(np.linspace(-dim0_offset, dim0_offset, n)):\n",
    "    for j, yval in enumerate(np.linspace(-dim1_offset, dim1_offset, n)):\n",
    "        \n",
    "        offset = pca.inverse_transform(np.array([[xval, yval]]))\n",
    "        new_z = z + torch.tensor(offset).float()\n",
    "\n",
    "        new_image = model.decoder(new_z.to(device))\n",
    "        new_image = new_image.reshape(1, image_size, image_size).detach().cpu()\n",
    "        axes[i, j].imshow(new_image.squeeze(), cmap='gray')\n",
    "        axes[i, j].set_title(f\"({xval:.2f}, {yval:.2f})\")\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
